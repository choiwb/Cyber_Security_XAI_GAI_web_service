{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerebras GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-111M\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-111M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mps device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How many enterprise Tactics IDs are in the Mitre Att&ck v13 matrix released in April 2023?\\n\\n Assistant:'\n",
    "# prompt = \"\"\" \n",
    "# Explain the following payload. Payload is GET /jenkins/postnuke/index.php?module=My_eGallery&do=showpic&pid=-1/**/AND/**/1=2/**/UNION/**/ALL/**/SELECT/**/0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,concat(0x3C7230783E,pn_uname,0x3a,pn_pass,0x3C7230783E),0,0,0/**/FROM/**/md_users/**/WHERE/**/pn_uid=$id/* HTTP/1.1 Content-Length: 36 Content-Type: application/x-www-form-urlencoded Host: www.test.go.kr Connection: Keep-Alive User-Agent: Mozilla/5.00 (Nikto/2.1.6) (Evasions:None).\\n\\nAssistant:\n",
    "# \"\"\"\n",
    "prompt = 'User: What is xss attack?\\n\\nAssistant: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is xss attack?\n",
      "\n",
      "Assistant: \n",
      "   - I want to know what is the problem.\n",
      "- I have a question about the xs attack.  I don't know how to solve it. I know I can't find a solution. But I'm not sure how I could solve this problem, I just want it to work.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device = 0)\n",
    "\n",
    "generated_text = pipe(prompt, max_length=256, do_sample=False, no_repeat_ngram_size=2)[0]\n",
    "print(generated_text['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch based inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on the encoded prompt\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        # input_ids=input_ids,\n",
    "        input_ids=input_ids.to(device),\n",
    "\n",
    "        do_sample=True,\n",
    "        top_p=0.75,\n",
    "        top_k=85,\n",
    "        temperature=1.99,\n",
    "        typical_p=1,\n",
    "        repetition_penalty=1.3,\n",
    "        max_length=250,  # The maximum number of tokens to generate\n",
    "        num_beams=5,    # The number of beams to use for beam search\n",
    "#         early_stopping=True,  # Stop generation when the model predicts an end-of-sequence token\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is xss attack?\n",
      "\n",
      "Assistant: \n",
      "\n",
      "Command Line: \n",
      "\n",
      "Line 2: Command Line\n",
      "\n",
      "Line 3: Commands Line 1--1\n",
      "\n",
      "Command Line 2: Command Line\n",
      "\n",
      "Command Line 3: Command Line\n",
      "\n",
      "Command Line 4: Command Line\n",
      "\n",
      "Command Line 8: Command Line\n",
      "\n",
      "Command Line 8: Command Line\n",
      "\n",
      "Command Line 9: Control Line\n",
      "\n",
      "Command Line 10: Control Line\n",
      "\n",
      "Command Line 11: Control Line\n",
      "\n",
      "Command Line 14: Command Line\n",
      "\n",
      "Command Line 12: Control Line\n",
      "\n",
      "Command Line 15: Control Line\n",
      "\n",
      "Command Line 16: Command Line\n",
      "\n",
      "Command Line 16: Command Line\n",
      "\n",
      "Command Line 17: Command Line\n",
      "\n",
      "Command Line 18: Command Line\n",
      "\n",
      "Command Line 20: Command Line\n",
      "\n",
      "Command Line 22: Command Line\n",
      "\n",
      "Command Line 24: Command Line\n",
      "\n",
      "Command Line 25: Command Line\n",
      "\n",
      "Command Line 26: Command Line\n",
      "\n",
      "Command Line 27: Command Line\n",
      "\n",
      "Command Line 28: Command Line\n",
      "\n",
      "Command Line 30: Command Line\n",
      "\n",
      "Command Line 31: Command Line\n",
      "\n",
      "Command Line 34: Command Line\n",
      "\n",
      "Command Line 36: Command Line\n",
      "\n",
      "Command Line 37:\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch based fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ngcsm/.cache/huggingface/datasets/json/default-674a5955815d9130/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002995014190673828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 17,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25938eb7f024ba786bb3d607b20cce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset('json', data_files='security_base_sample.json', field='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output', 'input'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_len = 1024\n",
    "\n",
    "def generate_prompt(entry):\n",
    "    if entry['input']:\n",
    "        return f\"User: {entry['instruction']}: {entry['input']}\\n\\nAssistant: {entry['output']}\"\n",
    "    else:\n",
    "        return f\"User: {entry['instruction']}\\n\\nAssistant: {entry['output']}\"\n",
    "\n",
    "def tokenize(item, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        generate_prompt(item),\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ngcsm/.cache/huggingface/datasets/json/default-674a5955815d9130/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6423add41a120364.arrow and /home/ngcsm/.cache/huggingface/datasets/json/default-674a5955815d9130/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5d398ee80083a909.arrow\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0028150081634521484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 17,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 40,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002663135528564453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 17,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 3,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val = dataset[\"train\"].train_test_split(test_size=0.05, shuffle=True, seed=595)\n",
    "train_data = train_val[\"train\"].shuffle().map(tokenize)\n",
    "val_data = train_val[\"test\"].shuffle().map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ngcsm/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ngcsm/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'model' in globals(): \n",
    "    del model\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    'cerebras/Cerebras-GPT-111M',    \n",
    "    \n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "\n",
    "    device_map={'': 0}\n",
    "    # device = torch.device(\"cpu\")\n",
    "    # device_map = 'auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "model = peft.prepare_model_for_int8_training(model)\n",
    "\n",
    "model = peft.get_peft_model(model, peft.LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb \n",
    "\n",
    "output_dir = 'cerebras-gpt111m-finetune'\n",
    "\n",
    "use_wandb = True,\n",
    "wandb_run_name = f\"{output_dir}-{wandb.util.generate_id()}\"\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=output_dir\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    # per_device_train_batch_size=16, \n",
    "    per_device_train_batch_size=1, \n",
    "\n",
    "    gradient_accumulation_steps=8,  \n",
    "    num_train_epochs=30,  \n",
    "    learning_rate=1e-4, \n",
    "    # only be used on CUDA devices.\n",
    "    fp16=True,\n",
    "    \n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=10, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    output_dir=output_dir, \n",
    "    save_total_limit=3,\n",
    "\n",
    "    report_to=\"wandb\" if use_wandb else None,\n",
    "    run_name=wandb_run_name if use_wandb else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    # model=model, \n",
    "    model=model.to(device), \n",
    "    \n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_args, \n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=1, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhuam1216\u001b[0m (\u001b[33mngcsm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ngcsm/cti_xai/gpt_app/wandb/run-20230502_133104-x5qg01c8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/x5qg01c8' target=\"_blank\">cerebras-gpt111m-finetune-481smeld</a></strong> to <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune' target=\"_blank\">https://wandb.ai/ngcsm/cerebras-gpt111m-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/x5qg01c8' target=\"_blank\">https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/x5qg01c8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ngcsm/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:201: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇██</td></tr><tr><td>train/learning_rate</td><td>█▇▇▇▆▅▅▅▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>30.0</td></tr><tr><td>train/global_step</td><td>150</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.8227</td></tr><tr><td>train/total_flos</td><td>41467889848320.0</td></tr><tr><td>train/train_loss</td><td>3.30576</td></tr><tr><td>train/train_runtime</td><td>41.3936</td></tr><tr><td>train/train_samples_per_second</td><td>28.99</td></tr><tr><td>train/train_steps_per_second</td><td>3.624</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerebras-gpt111m-finetune-481smeld</strong> at: <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/x5qg01c8' target=\"_blank\">https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/x5qg01c8</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230502_133104-x5qg01c8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "result = trainer.train()\n",
    "model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)  # 토크나이저도 함께 저장\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch based fine tuning model load & inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(2048, 768)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-9): 10 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): MergedLinear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                (lora_A): Linear(in_features=768, out_features=16, bias=False)\n",
       "                (lora_B): Conv1d(16, 1536, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): GELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): CastOutputToFloat(\n",
       "        (0): Linear(in_features=768, out_features=50257, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config\n",
    "print(model.dtype)\n",
    "\n",
    "model.half()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-9): 10 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장된 모델 호출\n",
    "output_dir = 'cerebras-gpt111m-finetune'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(output_dir,\n",
    "            load_in_8bit=True, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={'':0} if torch.cuda.is_available() else 'auto')\n",
    "\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What is XSS (Cross Site Scripting) attack?'\n",
    "# prompt = 'How many enterprise Tactics IDs are in the Mitre Att&ck v13 matrix released in April 2023?'\n",
    "# prompt = \"\"\" \n",
    "#Explain the following payload. Payload is GET /jenkins/postnuke/index.php?module=My_eGallery&do=showpic&pid=-1/**/AND/**/1=2/**/UNION/**/ALL/**/SELECT/**/0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,concat(0x3C7230783E,pn_uname,0x3a,pn_pass,0x3C7230783E),0,0,0/**/FROM/**/md_users/**/WHERE/**/pn_uid=$id/* HTTP/1.1 Content-Length: 36 Content-Type: application/x-www-form-urlencoded Host: www.test.go.kr Connection: Keep-Alive User-Agent: Mozilla/5.00 (Nikto/2.1.6) (Evasions:None).\\n\\n Assistant:\n",
    "# \"\"\"\n",
    "prompt = \"\"\" \n",
    "what is ta0006 (credential access)?\n",
    "\"\"\"\n",
    "# prompt = 'What is TA0006 (Credential Access)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = transformers.GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.2,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    # num_beams=5,\n",
    "    \n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        # input_ids=input_ids,\n",
    "        input_ids=input_ids.to(device),\n",
    "\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config\n",
    "    )[0].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is ta0006 (credential access)?\n",
      "\n",
      "Assistant: The adversary is trying to steal account names and passwords.\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4394eee0fff6724ce505428e0bf45950592091024b5e821af64edaa1085e7223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
