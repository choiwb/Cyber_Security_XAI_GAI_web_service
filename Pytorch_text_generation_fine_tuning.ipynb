{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerebras GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-111M\",\n",
    "                                         max_position_embeddings = 2048,\n",
    "                                        ignore_mismatched_sizes = True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-111M\",\n",
    "                                max_length = 2048, max_position_embeddings = 2048,\n",
    "                                    ignore_mismatched_sizes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"cerebras/Cerebras-GPT-111M\",\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_length\": 2048,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": 3072,\n",
       "  \"n_layer\": 10,\n",
       "  \"n_positions\": 2048,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.28.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_index = 0 # index of the GPU device you want to use\n",
    "device = torch.device('cuda', device_index)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum split size to avoid memory fragmentation\n",
    "torch.backends.cuda.max_split_size_bytes = 128 * 1024 * 1024  # 128 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How many enterprise Tactics IDs are in the Mitre Att&ck v13 matrix released in April 2023?\\n\\n Assistant:'\n",
    "# prompt = \"\"\" \n",
    "# Explain the following payload. Payload is GET /jenkins/postnuke/index.php?module=My_eGallery&do=showpic&pid=-1/**/AND/**/1=2/**/UNION/**/ALL/**/SELECT/**/0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,concat(0x3C7230783E,pn_uname,0x3a,pn_pass,0x3C7230783E),0,0,0/**/FROM/**/md_users/**/WHERE/**/pn_uid=$id/* HTTP/1.1 Content-Length: 36 Content-Type: application/x-www-form-urlencoded Host: www.test.go.kr Connection: Keep-Alive User-Agent: Mozilla/5.00 (Nikto/2.1.6) (Evasions:None).\\n\\nAssistant:\n",
    "# \"\"\"\n",
    "prompt = 'User: What is xss attack?\\n\\nAssistant: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is xss attack?\n",
      "\n",
      "Assistant: \n",
      "   - I want to know what is the problem.\n",
      "- I have a question about the xs attack.  I don't know how to solve it. I know I can't find a solution. But I'm not sure how I could solve this problem, I just want it to work.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device = 0)\n",
    "\n",
    "generated_text = pipe(prompt, max_length=256, do_sample=False, no_repeat_ngram_size=2)[0]\n",
    "print(generated_text['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch based inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on the encoded prompt\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        # input_ids=input_ids,\n",
    "        input_ids=input_ids.to(device),\n",
    "\n",
    "        do_sample=True,\n",
    "        top_p=0.75,\n",
    "        top_k=85,\n",
    "        temperature=1.99,\n",
    "        typical_p=1,\n",
    "        repetition_penalty=1.3,\n",
    "        max_length=2048,  # The maximum number of tokens to generate\n",
    "        num_beams=5,    # The number of beams to use for beam search\n",
    "#         early_stopping=True,  # Stop generation when the model predicts an end-of-sequence token\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is xss attack?\n",
      "\n",
      "Assistant:  \n",
      "Cards,\n",
      "\n",
      "Gives me a bit more on how to run the game. In my opinion the game is really going to be extremely easy to find. You are able to do things with very high speed. You can learn how it's going to work if you can't have good control and you will know what to do.\n",
      "\n",
      "And just to give you all these tips in there are two steps to get the most out of your game.\n",
      "\n",
      "First: The first step is to put it all together and play it all up in one shot. Next, move it all up in one shot until it becomes a shot. This is exactly the same way as the first step.\n",
      "\n",
      "So, if you want a little bit more control than a few shots, you will be ready to play the second one. It is also a great idea to put both arms around the ball (not the ball) while playing the third one.\n",
      "\n",
      "Then, when you have the ball off the ball, you will almost certainly need to move the ball closer to the ball so it doesn't have to jump right in that position.\n",
      "\n",
      "Then, the ball is now going to come through the ball. This is really what you should do for those who like it so they can play it.\n",
      "\n",
      "This is the easiest part of the game that will help you achieve the most important goals. Once you get into the game where you play it, it's very simple. You can play it once again and you can play it again.\n",
      "\n",
      "Second: After you have had a bit of a problem with the ball coming up, you will start to play and it's always going to be a different game. And it will make you feel like you have done something with the ball. You have got to play it again.\n",
      "\n",
      "Third: Once you've played the ball on the ball, you'll probably need to play the ball again. You have got to do it again and you'll go back and try it again. When you get into the game you will start to see the ball from the ball at that moment. This is exactly what you need to do every time.\n",
      "\n",
      "So, I will try to do it with the ball again. Then, if you don't play the ball with the ball you'll lose the ball. So, if you're going to play the ball again, you will be ready to play the ball again.\n",
      "\n",
      "I am trying to play the ball again but I can't wait to play it again. At this point, I have not seen the ball yet.\n",
      "\n",
      "Fourth: Once you play the ball, you will need to play the ball again. You are ready to play the ball again.\n",
      "\n",
      "There are a few ways that you could play the ball before you play the ball. So, let's play about four shots at the same time. This is an easy thing to do because the ball is moving in a perfectly balanced way. That means that you have some difficulty with the ball moving in the correct direction and you can play it. If the ball has moved in the correct direction, it will move away from the ball to the opposite side of the ball. So, if it moves in the correct direction and you play it the same way, then the ball will move across the ball. This is the same thing that you can play the ball again. There are several ways you could play the ball without losing the ball. Just play the ball again!\n",
      "\n",
      "A:\n",
      "\n",
      "You can still play the ball for a bit longer than you did. So, you could play the ball again. You need to use a different type of weapon, but it's best to play the ball at a very short distance in the proper way. It would be great if you played it with a very low weapon. If you played it with a slightly high weapon, you could play it with any of the other weapons. So, if you played it with a bit of a high weapon, it might be more difficult for you to play the ball at the same location. You can play the ball with a similar weapon, which you can play with a higher weapon or with a very high weapon.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch based fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ngcsm/.cache/huggingface/datasets/json/default-99d06e9db9babc1b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0029256343841552734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 16,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaf3babce8940cd8060fe8ff61a1f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset('json', data_files='security_base_sample.json', field='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_len = 2048\n",
    "\n",
    "def generate_prompt(entry):\n",
    "    if entry['input']:\n",
    "        return f\"User: {entry['instruction']}: {entry['input']}\\n\\nAssistant: {entry['output']}\"\n",
    "    else:\n",
    "        return f\"User: {entry['instruction']}\\n\\nAssistant: {entry['output']}\"\n",
    "\n",
    "def tokenize(item, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        generate_prompt(item),\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ngcsm/.cache/huggingface/datasets/json/default-99d06e9db9babc1b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-101ac43da4056645.arrow and /home/ngcsm/.cache/huggingface/datasets/json/default-99d06e9db9babc1b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-0a3e7f6d279aa471.arrow\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0027265548706054688,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 16,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 85,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002652883529663086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 16,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 5,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val = dataset[\"train\"].train_test_split(test_size=0.05, shuffle=True, seed=595)\n",
    "train_data = train_val[\"train\"].shuffle().map(tokenize)\n",
    "val_data = train_val[\"test\"].shuffle().map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in globals(): \n",
    "    del model\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    'cerebras/Cerebras-GPT-111M',    \n",
    "    \n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "\n",
    "    device_map={'': 0}\n",
    "    # device = torch.device(\"cpu\")\n",
    "    # device_map = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "model = peft.prepare_model_for_int8_training(model)\n",
    "\n",
    "model = peft.get_peft_model(model, peft.LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb \n",
    "\n",
    "output_dir = 'cerebras-gpt111m-finetune'\n",
    "\n",
    "use_wandb = True,\n",
    "wandb_run_name = f\"{output_dir}-{wandb.util.generate_id()}\"\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=output_dir\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    # per_device_train_batch_size=16, \n",
    "    per_device_train_batch_size=1, \n",
    "\n",
    "    gradient_accumulation_steps=8,  \n",
    "    # 학습 횟수 20 이상 시, 에러 발생 !!!!\n",
    "    num_train_epochs=19,  \n",
    "    learning_rate=1e-4, \n",
    "    # only be used on CUDA devices.\n",
    "    fp16=True,\n",
    "    \n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=10, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    output_dir=output_dir, \n",
    "    save_total_limit=3,\n",
    "\n",
    "    report_to=\"wandb\" if use_wandb else None,\n",
    "    run_name=wandb_run_name if use_wandb else None,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    # model=model, \n",
    "    model=model.to(device), \n",
    "\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_args, \n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=1, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhuam1216\u001b[0m (\u001b[33mngcsm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ngcsm/cti_xai/gpt_app/wandb/run-20230503_151352-2ydkjspg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/2ydkjspg' target=\"_blank\">cerebras-gpt111m-finetune-b0lurtpn</a></strong> to <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune' target=\"_blank\">https://wandb.ai/ngcsm/cerebras-gpt111m-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/2ydkjspg' target=\"_blank\">https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/2ydkjspg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 01:22, Epoch 17/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>17.88</td></tr><tr><td>train/global_step</td><td>190</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0342</td></tr><tr><td>train/total_flos</td><td>148491301109760.0</td></tr><tr><td>train/train_loss</td><td>0.3358</td></tr><tr><td>train/train_runtime</td><td>83.5317</td></tr><tr><td>train/train_samples_per_second</td><td>19.334</td></tr><tr><td>train/train_steps_per_second</td><td>2.275</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerebras-gpt111m-finetune-b0lurtpn</strong> at: <a href='https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/2ydkjspg' target=\"_blank\">https://wandb.ai/ngcsm/cerebras-gpt111m-finetune/runs/2ydkjspg</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230503_151352-2ydkjspg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "result = trainer.train()\n",
    "model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)  # 토크나이저도 함께 저장\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch based fine tuning model load & inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-9): 10 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config\n",
    "print(model.dtype)\n",
    "\n",
    "model.half()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-9): 10 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장된 모델 호출\n",
    "output_dir = 'cerebras-gpt111m-finetune'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(output_dir,\n",
    "            load_in_8bit=True, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={'':0} if torch.cuda.is_available() else 'auto'\n",
    "            )\n",
    "\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What is XSS (Cross Site Scripting) attack?'\n",
    "prompt = 'How many enterprise Tactics IDs are in the Mitre Att&ck v13 matrix released in April 2023?'\n",
    "# prompt = \"\"\" \n",
    "#Explain the following payload. Payload is GET /jenkins/postnuke/index.php?module=My_eGallery&do=showpic&pid=-1/**/AND/**/1=2/**/UNION/**/ALL/**/SELECT/**/0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,concat(0x3C7230783E,pn_uname,0x3a,pn_pass,0x3C7230783E),0,0,0/**/FROM/**/md_users/**/WHERE/**/pn_uid=$id/* HTTP/1.1 Content-Length: 36 Content-Type: application/x-www-form-urlencoded Host: www.test.go.kr Connection: Keep-Alive User-Agent: Mozilla/5.00 (Nikto/2.1.6) (Evasions:None).\\n\\n Assistant:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = 'What is TA0006 (Credential Access)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "# input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = transformers.GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.2,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    # num_beams=5,\n",
    "    \n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        # input_ids=input_ids,\n",
    "        input_ids=input_ids.to(device),\n",
    "\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config\n",
    "    )[0].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many enterprise Tactics IDs are in the Mitre Att&ck v13 matrix released in April 2023?\n",
      "\n",
      "Assistant: as of the mitre att&ck v13 framework, there are 14 enterprise tactics in the matrix. these tactics represent the various phases of a cyber attack life cycle. each tactic consists of multiple techniques that adversaries use to achieve their objectives.\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4394eee0fff6724ce505428e0bf45950592091024b5e821af64edaa1085e7223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
