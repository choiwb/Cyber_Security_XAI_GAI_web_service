{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "train_data_path = 'YOUR JSON FILE PATH'\n",
    "\n",
    "# JSON 파일 불러오기\n",
    "with open(train_data_path) as f:\n",
    "   train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD 데이터셋을 불러오는 함수\n",
    "def load_squad_data(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        squad_data = json.load(f)[\"data\"]\n",
    "    return squad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum split size to avoid memory fragmentation\n",
    "torch.backends.cuda.max_split_size_bytes = 128 * 1024 * 1024  # 128 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD 데이터셋과 DistilBERT의 tokenizer, 모델을 불러옴\n",
    "squad_data = load_squad_data(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "gpu_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                   max_length = 1024, max_position_embeddings = 1024, ignore_mismatched_sizes = True\n",
    "                                   ).to(device)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", max_length = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question = \"\"\"\n",
    "get /news/news_video/?pageid=3&mod=\"><script%20>alert in the case of the corresponding waf device payload, please write in one sentence or less, which type of attack it corresponds to.\n",
    "\"\"\"\n",
    "\n",
    "context = \"\"\"\n",
    "if any string appears between script and alert in waf payload, it is a type of xss (cross site scripting) attack.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "answering = pipeline('question-answering', model = cpu_model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = answering(question = new_question, context = context)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD 데이터셋을 DistilBERT의 입력 형식에 맞게 변환하는 함수\n",
    "def convert_squad_data_to_features(squad_data, tokenizer, max_seq_length):\n",
    "    features = []\n",
    "    for article in squad_data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                print(qas_id)\n",
    "                question = qa[\"question\"]\n",
    "                print(question)\n",
    "                answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                print(answer_text)\n",
    "                start_position = qa[\"answers\"][0][\"answer_start\"]\n",
    "                print(start_position)\n",
    "                end_position = start_position + len(answer_text)\n",
    "                print(end_position)\n",
    "\n",
    "                # context와 question을 DistilBERT의 입력 형식에 맞게 tokenize\n",
    "                encoded_dict = tokenizer(question, context, max_length=max_seq_length, padding=\"max_length\",\n",
    "                                         # 지정된 token 수 (예, 1024개) 초과 시, 자름\n",
    "                                         truncation=True, return_offsets_mapping=True, return_token_type_ids = True)\n",
    "                print(encoded_dict)\n",
    "                # answer의 시작 위치와 끝 위치를 토큰 단위로 변환\n",
    "                token_start_position = 0\n",
    "                token_end_position = 0\n",
    "                for i, offset in enumerate(encoded_dict[\"offset_mapping\"]):\n",
    "                    if offset[0] <= start_position and offset[1] > start_position:\n",
    "                        token_start_position = i\n",
    "                    if offset[0] < end_position and offset[1] >= end_position:\n",
    "                        token_end_position = i\n",
    "\n",
    "                # feature 추가\n",
    "                input_ids = encoded_dict[\"input_ids\"]\n",
    "                attention_mask = encoded_dict[\"attention_mask\"]\n",
    "                # token_type_ids = encoded_dict.token_type_ids()\n",
    "                token_type_ids = encoded_dict['token_type_ids']\n",
    "\n",
    "                features.append((input_ids, attention_mask, token_type_ids, token_start_position, token_end_position))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 sequence의 최대 길이\n",
    "max_seq_length = 1024\n",
    "\n",
    "# 데이터셋을 feature로 변환\n",
    "features = convert_squad_data_to_features(squad_data, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_model.config.max_length = max_seq_length\n",
    "# gpu_model.config.max_position_embeddings = 1024\n",
    "gpu_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature를 torch tensor로 변환\n",
    "input_ids = torch.tensor([f[0] for f in features], dtype=torch.long)\n",
    "attention_mask = torch.tensor([f[1] for f in features], dtype=torch.long)\n",
    "token_type_ids = torch.tensor([f[2] for f in features], dtype=torch.long)\n",
    "start_positions = torch.tensor([f[3] for f in features], dtype=torch.long)\n",
    "end_positions = torch.tensor([f[4] for f in features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch: [(input_ids, attention_masks, token_type_ids, start_positions, end_positions), ...]\n",
    "    input_ids = torch.tensor([item[0] for item in batch])\n",
    "    attention_masks = torch.tensor([item[1] for item in batch])\n",
    "    token_type_ids = torch.tensor([item[2] for item in batch])\n",
    "    start_positions = torch.tensor([item[3] for item in batch])\n",
    "    end_positions = torch.tensor([item[4] for item in batch])\n",
    "    return input_ids, attention_masks, token_type_ids, start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer와 learning rate 설정\n",
    "optimizer = AdamW(gpu_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(features, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "gpu_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# train loop 설정\n",
    "train_loss = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # 데이터 준비\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0].to(device),\n",
    "            \"attention_mask\": batch[1].to(device),\n",
    "            # \"token_type_ids\": batch[2].to(device),\n",
    "            \"start_positions\": batch[3].to(device),\n",
    "            \"end_positions\": batch[4].to(device)\n",
    "        }\n",
    "\n",
    "        # forward 수행\n",
    "        gpu_model.train()\n",
    "        outputs = gpu_model(**inputs)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # backward 수행\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # loss 계산\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_loss.append(epoch_loss / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터셋 로딩\n",
    "eval_features = convert_squad_data_to_features(squad_data, tokenizer, max_seq_length)\n",
    "eval_dataloader = DataLoader(eval_features, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "gpu_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = 0\n",
    "for batch in eval_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0].to(device),\n",
    "            \"attention_mask\": batch[1].to(device),\n",
    "            \"start_positions\": batch[3].to(device),\n",
    "            \"end_positions\": batch[4].to(device)\n",
    "        }\n",
    "        outputs = gpu_model(**inputs)\n",
    "        # print('@@@@@@@@@@@@')\n",
    "        # print(outputs)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "        start_positions, end_positions = inputs[\"start_positions\"], inputs[\"end_positions\"]\n",
    "        # batch_loss = loss_fn(start_logits, end_logits, start_positions, end_positions)\n",
    "        batch_loss = loss_fn(start_logits, start_positions) + loss_fn(end_logits, end_positions)\n",
    "\n",
    "        eval_loss += batch_loss.item()\n",
    "\n",
    "eval_loss /= len(eval_dataloader)\n",
    "print(\"Eval Loss:\", eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question = \"\"\"\n",
    "get /user?select%20from in the case of the corresponding waf device payload, please write in one sentence or less, which type of attack it corresponds to.\n",
    "\"\"\"\n",
    "\n",
    "context = \"\"\"\n",
    "if any string appears between select and from in waf payload, it is a type of sql injection attack.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_index = 0 # index of the GPU device you want to use\n",
    "device = torch.device('cuda', device_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answering = pipeline('question-answering', model = gpu_model, tokenizer = tokenizer, device = device)\n",
    "result = answering(question = new_question, context = context)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
